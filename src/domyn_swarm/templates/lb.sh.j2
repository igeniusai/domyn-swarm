#!/bin/bash
#SBATCH --account={{ cfg.backend.account }}
{% if cfg.backend.mail_user %}
#SBATCH --mail-type=END,FAIL
#SBATCH --mail-user={{ cfg.backend.mail_user }}
{% endif %}
#SBATCH --qos={{ cfg.backend.qos }}

#SBATCH --exclusive
#SBATCH --job-name={{ job_name }}-lb
#SBATCH --partition={{ cfg.backend.partition }}
#SBATCH --nodes=1
#SBATCH --cpus-per-task={{ cfg.backend.endpoint.cpus_per_task }}
#SBATCH --mem={{ cfg.backend.endpoint.mem }}
#SBATCH --threads-per-core={{ cfg.backend.endpoint.threads_per_core }}
#SBATCH --time={{ cfg.backend.endpoint.wall_time }}
#SBATCH --output={{ swarm_directory }}/logs/endpoint/lb.out
#SBATCH --error={{ swarm_directory }}/logs/endpoint/lb.err

set -eu
REPLICAS={{ replicas }}
HOST_DIR="{{ swarm_directory }}"
SERVING_DIR="$HOST_DIR/serving"
WAIT_SEC={{ cfg.wait_endpoint_s | default(1800) }}
NGINX_IMG={{ cfg.backend.endpoint.nginx_image }}
PYTHON_IMG={{ cfg.image }}
INSTANCE_NAME="lb_${DEP_JOBID}_${SLURM_JOB_ID}"
PORT={{ cfg.backend.endpoint.port }}

COLLECTOR_PORT="${COLLECTOR_PORT:-9100}"
COLLECTOR_HOST="$(hostname -f)"

singularity exec --writable-tmpfs --bind "{{ collector_script_path }}:/opt/watchdog_collector.py,$HOST_DIR" "$PYTHON_IMG" \
  python3 /opt/watchdog_collector.py \
  --db "$HOST_DIR/watchdog.db" \
  --host "0.0.0.0" \
  --port "$COLLECTOR_PORT" \
  >>"$HOST_DIR/logs/collector.log" 2>&1 &
COLLECTOR_PID=$!

{
    echo "COLLECTOR_HOST=$COLLECTOR_HOST"
    echo "COLLECTOR_PORT=$COLLECTOR_PORT"
} > "$HOST_DIR/serving/collector.env"

echo "[lb] started collector on $COLLECTOR_HOST:$COLLECTOR_PORT with PID $COLLECTOR_PID"

trap 'echo "[lb] shutting down collector"; kill $COLLECTOR_PID' EXIT

echo "[lb] waiting for $REPLICAS head files from job $DEP_JOBID"

t0=$(date +%s)
files=()
while true; do
    files=( $(ls "$SERVING_DIR/"*".head" 2>/dev/null || true) )
    {% raw %}
    if (( ${#files[@]} == REPLICAS )); then
        break
    fi
    {% endraw %}
    if (( $(date +%s) - t0 > WAIT_SEC )); then
    {% raw %}
        echo "[lb] timeout: only ${#files[@]}/$REPLICAS replicas became healthy" >&2
    {% endraw %}
        exit 1
    fi
    sleep 5
done

echo "[lb] all replicas ready → ${files[*]}"

generate_nginx_conf() {
    # Collect all replica-*.head files (each contains "host:port")
    local files=("$SERVING_DIR"/replica-*.head)

    {
        echo "pid /tmp/nginx.pid;"
        echo "events { worker_connections 100000; }"
        echo "http {"
        echo "  upstream llm {"
        echo "    least_conn;"

        # Backends: one server line per replica, sorted for stable config
        for f in "${files[@]}"; do
            [ -f "$f" ] || continue
            addr=$(<"$f")  # "host:port"
            echo "    server $addr max_fails=2 fail_timeout=10s;"
        done

        echo "  }"

        {% if cfg.backend.requires_ray %}
        # TODO: support multiple ray heads
        ray_head_file="$HOST_DIR/serving/replica-0.head"
        if [ -f "$ray_head_file" ]; then
            ray_host=$(sed -n '1s/:.*//p' "$ray_head_file")
            echo "  upstream ray {"
            echo "    server $ray_host:{{ cfg.backend.ray_dashboard_port }};"
            echo "    keepalive 8;"
            echo "  }"

            echo "  upstream ray_control {"
            echo "    server $ray_host:{{ cfg.backend.ray_port }};"
            echo "    keepalive 8;"
            echo "  }"
        fi
        {% endif %}

        cat <<CONF
  server {
    listen $PORT;
    location / {
      proxy_pass http://llm;
      proxy_connect_timeout       {{ cfg.backend.endpoint.nginx_timeout }};
      proxy_send_timeout          {{ cfg.backend.endpoint.nginx_timeout }};
      proxy_read_timeout          {{ cfg.backend.endpoint.nginx_timeout }};
      send_timeout                {{ cfg.backend.endpoint.nginx_timeout }};
      proxy_set_header Connection "";
      sendfile on;
      tcp_nopush on;
      tcp_nodelay on;
      keepalive_timeout 65;
      {% if cfg.backend.endpoint.enable_proxy_buffering %}
      # ───── Proxy Response Buffering ─────
      proxy_buffer_size 64k;
      proxy_buffers 16 512k;
      proxy_busy_buffers_size 1m;
      proxy_max_temp_file_size 0;
      # ───── Client Request Buffering ─────
      client_max_body_size 50m;
      client_body_buffer_size 1m;
      {% else %}
      proxy_buffering off;
      proxy_request_buffering off;
      {% endif %}

      proxy_next_upstream error timeout http_502 http_504;
      proxy_next_upstream_tries 3;
    }
    location = /health {
      proxy_pass http://llm/health;
      proxy_connect_timeout 2s;
      proxy_read_timeout 2s;
      proxy_cache_bypass \$http_upgrade;
    }
    {% if cfg.backend.requires_ray %}
    location ^~ /ray/ {
      proxy_pass              http://ray/;
      proxy_read_timeout      300s;
      proxy_connect_timeout   60s;
      proxy_set_header Host   \$host;
      proxy_set_header X-Real-IP \$remote_addr;
    }
    location = /ray/dashboard {
      return 301 /ray/dashboard/;
    }
    {% endif %}
  }
}
CONF
    }
}

generate_nginx_conf > $HOST_DIR/serving/nginx.conf

CACHE_DIR="$TMPDIR/cache"
mkdir -p "$CACHE_DIR"

singularity instance start --writable-tmpfs \
    -B "$HOST_DIR/serving/nginx.conf":/etc/nginx/nginx.conf:ro \
    -B "$CACHE_DIR":/var/cache/nginx \
    -B "$HOST_DIR/logs/endpoint:/var/log/nginx" \
    "$NGINX_IMG" "$INSTANCE_NAME"

trap 'echo "[lb] caught SIGTERM → shutting down"; singularity instance stop "$INSTANCE_NAME" || true; exit 0' SIGTERM

echo "[lb] starting reload loop; watching $SERVING_DIR/replica-*.head"

while true; do
  tmp_conf="$SERVING_DIR/nginx.conf.tmp"
  generate_nginx_conf > "$tmp_conf"

  # If config changed (or nginx.conf doesn't exist yet), replace & reload
  if ! cmp -s "$tmp_conf" "$SERVING_DIR/nginx.conf"; then
    mv "$tmp_conf" "$SERVING_DIR/nginx.conf"
    echo "[lb] updated nginx.conf; reloading nginx..."
    # Reload inside the instance
    singularity exec "instance://$INSTANCE_NAME" nginx -s reload || \
      echo "[lb] WARNING: nginx reload failed" >&2
  else
    rm -f "$tmp_conf"
  fi

  sleep 5
done
