<p align="center">
  <picture>
      <source srcset="static/domyn-swarm-logo-white.svg" media="(prefers-color-scheme: dark)">
      <source srcset="static/domyn-swarm-logo-primary.svg" media="(prefers-color-scheme: light)">
      <img src="static/domyn-swarm-logo-primary.svg" alt="domyn-swarm" height="100">
   </picture>
</p>

> A simple, batteriesâ€‘included CLI and Python library for launching **LLM serving endpoints** and running **highâ€‘throughput batch jobs** against them. Firstâ€‘class support for **Slurm** (HPC) and **NVIDIA DGX Cloud Lepton**.

<p align="center">
<img src="https://github.com/igeniusai/domyn-swarm/actions/workflows/ci.yaml/badge.svg" alt="CI">
<img src="https://img.shields.io/badge/python-3.10%7C3.11%7C3.12%7C3.13-brightgreen?style=flat&logoColor=green" alt="Python">
<img src="https://img.shields.io/badge/License-Apache%202.0-blue" alt="License - Apache 2.0">
<img src="https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json" alt="Ruff">
<img src="https://microsoft.github.io/pyright/img/pyright_badge.svg" alt="Pyright">
</p>

---

## Why Domynâ€‘Swarm?

Domynâ€‘Swarm gives you a **single, consistent workflow** to:

* stand up a **scalable LLM endpoint** (vLLM, OpenAIâ€‘compatible),
* **submit jobs/scripts** that call that endpoint with **checkpointing, retries, and concurrency**, and
* **tear it all down** cleanly â€” across HPC (Slurm) and cloud (Lepton) backends.

Itâ€™s designed for **fast evaluation loops**, **robust batch inference**, and **easy backend extension**.

---

## Features

* **One CLI** for **up â†’ submit â†’ status â†’ down** across platforms
* **Serving/Compute backends** behind clean protocols â†’ easy to add new targets (e.g., AzureML)
* **Health checks & readiness**:
  * Slurm: array replicas + LB, HTTP probe on `/v1/models`
  * Lepton: deployment state polling
* **SwarmJob API** (DataFrame in â†’ DataFrame out)
  * Builtâ€‘in **batching**, **bounded concurrency**, **tenacity retries**, **checkpointing** (Parquet)
  * **Compat layer** for older job shape
* **Script runner** (submit any Python file to the compute backend)
* **State persistence** (using SQLite to store your swarms state)
* **Optional extras**: `pip install domyn-swarm[lepton]` to enable DGX Cloud Lepton

---

## Supported backends

* **Slurm** (HPC) â€” uses singularity containers and job arrays to run model replicas
* **NVIDIA DGX Cloud Lepton** â€” Endpoint + Batch Job via Lepton SDK (optional extra)

---

## Installation

**PyPI (once published):**

> [!NOTE]
> We still haven't published the package on PyPI, but it will soon be available


```bash
pip install domyn-swarm
# Optional Lepton support
pip install 'domyn-swarm[lepton]'
```


**From source (GitHub):**

```bash
pip install git+ssh://git@github.com/igeniusai/domyn-swarm.git
# or with uv
uv pip install git+ssh://git@github.com/igeniusai/domyn-swarm.git # git+ssh://git@github.com/igeniusai/domyn-swarm.git[lepton]
uv add git+ssh://git@github.com/igeniusai/domyn-swarm.git # Add --extra lepton
uv tool install --from git+ssh://git@github.com/igeniusai/domyn-swarm.git --python 3.12 domyn-swarm
```

> Lepton users: install the extra and run `lep login` to initialize credentials.

---

## Quickstart

### 1) Prepare a YAML config

```yaml
# config.yaml
model: "HuggingFaceTB/SmolLM3-3B-Base"
gpus_per_replica: 16
replicas: 2
backend:
  type: slurm
  partition: partition_name    # your HPC partition
  account: account_name          # your HPC account
  qos: qos_name             # qos for cluster + jobs
```

> **Note**: `model` can be an HF ID or a local path. If HF, ensure itâ€™s downloaded to your `HF_HOME`.

### 2) Launch a swarm

```bash
domyn-swarm up -c config.yaml
```

This submits:

* an **array job** with 2 cluster replicas (vLLM servers)
* a **loadâ€‘balancer** job (Nginx) that waits on all replicas
* updates/creates a local **SQLite** state record for the swarm

### 3) Submit a typed job (DataFrame â†’ DataFrame)

```bash
domyn-swarm job submit \
  --name my-swarm-name \
  --job-kwargs '{"temperature":0.3,"checkpoint_interval":16,"max_concurrency":8,"retries":2}' \
  --input examples/data/chat_completion.parquet \
  --output results.parquet
```

Under the hood, this spawns a driver that:

* reads `ENDPOINT=http://<endpoint-node>:9000`
* runs `python -m domyn_swarm.jobs.run ...` via `srun` (Slurm) or the platform equivalent
* streams promptsâ†’answers with batching, backoff retries, checkpointing

### 4) Submit a freeâ€‘form Python script

```bash
domyn-swarm job submit-script \
  --name my-swarm-name \
  examples/my_custom_driver.py -- --verbose --foo bar
```

### 5) Check status (Slurm)

```bash
domyn-swarm status --name my-swarm-name
```

### 6) Shut down

```bash
domyn-swarm down my-swarm-name
```

---

## Lepton (DGX Cloud) configuration

Install the extra and login first:

```bash
lep login
```

Example configuration:

```yaml
backend:
  type: lepton
  workspace_id: workspace_id
  endpoint:
    image: vllm/vllm-openai
    allowed_dedicated_node_groups:
      - nodegroup-xx
    resource_shape: gpu.4xh200
    env:
      HF_HOME: /mnt/lepton-shared-fs/hf_home/
  job:
    allowed_dedicated_node_groups:
      - nodegroup-xx
    image: igeniusai/domyn-swarm:latest
```

> Secrets: the endpoint can be tokenâ€‘protected; Domynâ€‘Swarm stores the secret name in the serving handle and passes it to jobs as env (`DOMYN_SWARM_API_TOKEN` or secret ref).

---

## Commands

```
Usage: domyn-swarm [OPTIONS] COMMAND [ARGS]â€¦

Options:
  --install-completion    Install shell completion
  --show-completion       Show existing completion
  --help                  Show this help and exit

Commands:
  version   Show the version of the domyn-swarm CLI
  up        Launch a new swarm allocation
  status    Check the status of the swarm allocation
  down      Shut down a swarm allocation
  submit    Submit a workload to a Domyn-Swarm allocation.
  pool      Submit a pool of swarm allocations from a YAML config.
  init      Initialize a new Domyn-Swarm configuration.
```

### `domyn-swarm up`

Start a new allocation:

```bash
domyn-swarm up -c config.yaml \
  --name my-beautiful-llm-swarm \
  --replicas 3 \  # I'm overriding what's in the configuration file
  --reverse-proxy
```

* `-c/--config` â€” path to your YAML
* `-n/--name` - Name of the swarm allocation. If not provided, a random name will be generated.
* `-r/--replicas` â€” override number of replicas
* `--reverse-proxy` â€” (TBD) launch an Nginx running on the login node you're logged, so that you can access Ray dashboard via SSH tunneling


### `domyn-swarm down`

```bash
domyn-swarm down my-beautiful-llm-swarm
```

Take a Swarm name as input. It stops the LB and all replica jobs via `scancel`.

### `domyn-swarm job submit`

Typed DataFrame â†’ DataFrame jobs:

```bash
domyn-swarm job submit \
  my_module:CustomCompletionJob \
  --name my-swarm-name \
  --job-kwargs '{"temperature":0.2,"checkpoint_interval":16}' \
  --input prompts.parquet \
  --output answers.parquet
```

* `<module>:<ClassName>` implementing `SwarmJob`, defaults to `domyn_swarm.jobs:ChatCompletionJob`
* **--input** / **--output** â€” Parquet files on shared filesystem
* **--job-kwargs** â€” JSON for the jobâ€™s constructor
* **--config** or **--state** (one only)  -  the definition or state of the cluster where the job will be submitted
* **--checkpoint-interval** - batch size of the requests to be sent to be processed. Once a batch has finished processing, the checkpoint will be updated
* **--max-concurrency** - Number of concurrent requests to process
* **--retries** - Number of retries for failed requests
* **--num-threads** - How many threads should be used by the driver to run the job
* **--limit** / **-l** - Limit the size to be read from the input dataset. Useful when debugging and testing to reduce the size of the dataset
* **--detach** - Detach the job from the current terminal, running in a different process (PID will be printed)


Internally uses checkpointing, batching, and retry logic.


### `domyn-swarm job submit-script`

Free-form script on the head node:

```bash
domyn-swarm job submit-script \
  --name my-swarm-name \
  path/to/script.py -- --foo 1 --bar 2
```

* **script\_file**: your `.py` file (must exist)
* **--config** or **--jobid** (one only)
* **argsâ€¦** after `--` are forwarded to your script

---

### Core Configuration: `DomynLLMSwarmConfig`

All runtime options for the swarm launcher live in a single YAML file that is loaded into the `DomynLLMSwarmConfig` dataclass.
Below is an overview of every field, its purpose, and the default that will be used if you omit it.

| Field                         | Type           | Default                                      | Purpose                                                                                                                                      |                                                                   |
| ----------------------------- | -------------- | -------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **model**                     | `str`          | **required**                                 | HF model ID or local path. Passed verbatim to `vllm serve`; must resolve to a local directory or an offline Hugging Face model in `HF_HOME`. |                                                                   |
| **revision**                  | `str          \| null`                                       | `null`                                                                                                                                       | Git tag/commit for the model (if using HF).                       |
| **replicas**                  | `int`          | `1`                                          | How many *independent* vLLM clusters to launch (useful for A/B tests).                                                                       | |
| **gpus_per_replica**          | `int`          | `4`                                          | How many GPUs should the replica use? This parameter will also be used to set `--tensor-parallel-size`| |
| **gpus\_per\_node**           | `int`          | `4`                                          | GPUs allocated on each worker node. It depends on the platform: our Slurm infrastructure used 4 GPUs nodes, but yours may have 8.                                                                                                          |                                                                   |
| **nodes**                     | `int`          | `math.ceil(replicas / replicas_per_node)` or `math.ceil((replicas * gpus_per_replica) / gpus_per_node)` for multi-gpu multi-node clusters                                          | Worker nodes per replica (one vLLM server per node).                                                                                         |                                                                   |
| **cpus\_per\_task**           | `int`          | `32 // replicas_per_node` or `32` for multi-gpu multi-node clusters                                   | vCPUs reserved per SLURM task.                                                                                                               |
| **replicas\_per\_node**       | `int`          | `gpus_per_node // gpus_per_replica` if `gpus_per_replica` <= `gpus_per_node` else `None` | How many model instances can share the same node (you won't need to set this unless you want multiple replicas per GPU, e.g. 2 replicas for each gpu) |
| **image**               | `str          \| pathlib.Path`                               | `null`                                                                                   | Can be either the path to a Singularity Image to be used with a Slurm backend or a docker image, used with                                 |
| **args**                | `str`          | `""`                                         | Extra CLI flags passed verbatim to `python -m vllm.entrypoints.openai.api_server â€¦`.                                                         |                                                                   |
| **port**                | `int`          | `8000`                                       | Port where each workerâ€™s OpenAI-compatible API listens.                                                                                      |                                                                   |
| **home\_directory**           | `pathlib.Path` | value of `DOMYN_SWARM_HOME`                            | Root folder for swarm state (auto-generated inside CWD).                                                                                     |                                                                   |
| **env**                 |  `dict` | `null` | A yaml dict of key values that will be set as environment variables |
| **backend**                    | `BackendConfig` | *see below*                                  | Backend specific configurations, either Slurm or Lepton                                                             |                                                                   |

### Backend Configuration: `BackendConfig`

#### SlurmConfig

| Field                         | Type           | Default                                      | Purpose                                                                                                                                      |                                                                   |
| ----------------------------- | -------------- | -------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| **type** | `Literal["slurm"]` | "slurm" | Type of backend |
| **partition**                 | `str`          | `null`                           | SLURM partition to submit to.                                                                                                                |                                                                   |
| **account**                   | `str`          | `null`                               | SLURM account / charge code.                                                                                                                 |                                                                   |
| **qos** | `str` | `null` | SLURM qos where the cluster and load balancer jobs will be submitted |
| **requires_ray**              | `bool` | `null` | Set automatically to enforce the usage of Ray + vLLM for multi-node multi-gpu clusters |
| **ray\_port**                 | `int`          | `6379`                                       | Port for Rayâ€™s GCS/head node inside each replica.                                                                                            |                                                                   |
| **ray\_dashboard\_port**      | `int`          | `8265`                                       | Ray dashboard (optional).                                                                                                                    |                                                                   |
| **venv\_path**                | `pathlib.Path \| null`                                       | `null`                                                                                                                                       | Virtual-env used by the *driver* process (not the containers).    |
| **time\_limit**               | `str`          | `"36:00:00"`                                 | Overall SLURM wall-clock limit for the allocation.                                                                                           |                                                                   |
| **exclude\_nodes**            | `str          \| null`                                       | `null`                                                                                                                                       | Nodes to exclude, e.g. `"node[001-004]"` (pass-through to SLURM). |
| **node\_list**                | `str          \| null`                                       | `null`                                                                                                                                       | Explicit node list, e.g. `"node[005-008]"`.                       |
| **log\_directory**            | `pathlib.Path \| null`                                       | `<home_directory>/logs`                                                                                                                      | Where SLURM stdout/stderr files are written.                      |
| **template\_path**            | `pathlib.Path` | *(auto-filled)*                              | Internal path of the Jinja2 SLURM script template; no need to modify.                                                                        |                                                                   |
| **nginx\_template\_path**     | `pathlib.Path` | *(auto-filled)*                              | Jinja2 template for the NGINX config. | |
| **mail_user**                 | `str` | `null` | Send Slurm END,FAIL signal notification about the resources deployed by domyn-swarm (job and cluster) | myemail@gmail.com |
| **endpoint**                  | `SlurmEndpointConfig` | `null` | |

#### SlurmEndpointConfig

| Field                  | Type  | Default      | Purpose                                                         |
| ---------------------- | ----- | ------------ | --------------------------------------------------------------- |
| **cpus\_per\_task**    | `int` | `32`          | vCPUs for the driver process (launches and monitors the swarm). |
| **mem**                | `str` | `"16GB"`     | Physical memory for the driver job.                             |
| **threads\_per\_core** | `int` | `1`          | SMT threads to request per physical core.                       |
| **wall\_time**         | `str` | `"24:00:00"` | SLURM time limit for the driver job.                            |
| **nginx\_image**       | `str \| pathlib.Path` | `null` | Path to a singularity image running NGINX as load balancer for the swarm.|
| **nginx_timeout**      | `str \| int` | "60s" | HTTP timeout for NGINX proxy requests to model replicas. |
| **port**           | `int` | `9000`       | External port exposed by the NGINX load balancer. |
| **poll_interval**  | `int` | `10` | Seconds between status checks while waiting for the load balancer to become ready. |


#### LeptonConfig

| Field          | Type                   | Default                  | Purpose                                                                                   |
| -------------- | ---------------------- | ------------------------ | ----------------------------------------------------------------------------------------- |
| `type`         | `Literal["lepton"]`    | *required*               | Discriminator to select the Lepton backend.                                               |
| `workspace_id` | `str`                  | *required*               | Lepton workspace identifier used for deployments and jobs.                                |
| `endpoint`     | `LeptonEndpointConfig` | `LeptonEndpointConfig()` | Serving endpoint configuration (image, shape, mounts, env, token secret).                 |
| `job`          | `LeptonJobConfig`      | `LeptonJobConfig()`      | Batch job configuration (image, shape, mounts, env).                                      |
| `env`          | `dict[str, str]`       | `{}`                     | Additional global environment variables applied to both endpoint and jobs where relevant. |

---


#### LeptonEndpointConfig

| Field                           | Type                | Default                     | Purpose                                                                                                 |
| ------------------------------- | ------------------- | --------------------------- | ------------------------------------------------------------------------------------------------------- |
| `image`                         | `str`               | `"vllm/vllm-openai:latest"` | Container image used to run the serving endpoint (vLLMâ€‘compatible OpenAI server).                       |
| `allowed_dedicated_node_groups` | `list[str] \| None` | `None`                      | Optional constraint to one or more Lepton dedicated node groups for the endpoint.                       |
| `resource_shape`                | `str`               | `"gpu.8xh200"`              | Lepton resource shape for endpoint replicas (e.g., GPU type/count and memory).                          |
| `allowed_nodes`                 | `list[str]`         | `[]`                        | Optional list of specific nodes (within a node group) where the endpoint is allowed to run.             |
| `mounts`                        | `list[MountLike]`   | `_default_mounts()`         | Filesystem mounts injected into the endpoint container (validated to Lepton `Mount` if SDK is present). |
| `env`                           | `dict[str, str]`    | `{}`                        | Environment variables for the endpoint container.                                                       |
| `api_token_secret_name`         | `str \| None`       | `None`                      | Name of the Lepton secret that holds the API token exposed to jobs/clients.                             |

**Notes**

* A validator normalizes `mounts` to real Lepton `Mount` instances when the Lepton SDK is available; otherwise accepts dicts.

---

#### LeptonJobConfig

| Field                           | Type                | Default                          | Purpose                                                                                            |
| ------------------------------- | ------------------- | -------------------------------- | -------------------------------------------------------------------------------------------------- |
| `allowed_dedicated_node_groups` | `list[str] \| None` | `None`                           | Optional constraint to one or more Lepton dedicated node groups for batch jobs.                    |
| `image`                         | `str`               | `"igeniusai/domyn-swarm:latest"` | Container image used by Domynâ€‘Swarm driver jobs submitted to Lepton.                               |
| `resource_shape`                | `str`               | `"gpu.8xh200"`                   | Lepton resource shape for the batch job execution.                                                 |
| `allowed_nodes`                 | `list[str]`         | `[]`                             | Optional list of specific nodes where the job is allowed to run.                                   |
| `mounts`                        | `list[MountLike]`   | `_default_mounts()`              | Filesystem mounts injected into the job container (validated to Lepton `Mount` if SDK is present). |
| `env`                           | `dict[str, str]`    | `{}`                             | Environment variables for the job container.                                                       |

**Notes**

* Same mounts normalization behavior as the endpoint config.

---


**Terminology**

* **MountLike**: either a Lepton SDK `Mount` object (when SDK is installed) or a dict with the same fields; validated at runtime.
* **Resource shape**: Lepton preset describing accelerator type/count and other resources (e.g., `gpu.4xh200`).


---

*Tip:* Any field omitted from your YAML file inherits the default above, so you can keep configuration files minimalâ€”only override what you need.

*Tip:* You can set some defaults for your specific environment by running `domyn-swarm init defaults`

---

## Environment variables

There is a variety of environment variables that you can set and will be picked up by `domyn-swarm` automatically

**Overview**
`Settings` centralizes configuration sourced from environment variables and optional `.env` files. By default it reads from:

* `.env` in the current working directory
* `~/.domyn_swarm/.env`

Environment variables use the prefix `DOMYN_SWARM_` (case-insensitive) **unless an explicit alias is defined** (see the table below). Values are parsed and validated via Pydantic.

| Name                    | Type                     | Purpose                                                              |
| ----------------------- | ------------------------ | -------------------------------------------------------------------- |
| `DOMYN_SWARM_LOG_LEVEL` | string                   | Global logging level (e.g., `DEBUG`, `INFO`, `WARNING`).             |
| `DOMYN_SWARM_HOME`      | path                     | Home/state directory for domynâ€‘swarm files (e.g., `~/.domyn_swarm`). |
| `DOMYN_SWARM_DEFAULTS`  | path (optional)          | Path to YAML with overridable defaults used by the defaults loader (e.g. `~/.domyn_swarm/defaults.yaml`).  |
| `API_TOKEN`             | secret string (optional) | API token to authenticate with the vLLMâ€‘compatible server.           |
| `DOMYN_SWARM_MAIL_USER` | string (optional)        | Email address for Slurm job notifications.                           |
| `LEPTONAI_API_TOKEN`    | secret string (optional) | Token for Lepton API authentication.                                 |
| `LEPTON_WORKSPACE_ID`   | string (optional)        | Default Lepton workspace ID used for deployments/jobs.               |

## Python API (Programmatic usage)

> [!NOTE]
> This API is in constant evolution and you can expect breaking changes up to the final stable release

In the [examples](examples/) folder, you can see some examples of programmatic usage of `DomynLLMSwarm` by instantiating a custom implementation of SwarmJob and how to run it via CLI or in a custom script: [examples/scripts/custom_main.py](examples/scripts/custom_main.py).

### Define a custom job

```python
import random
from typing import Any, List, Tuple

import pandas as pd
from domyn_swarm.jobs import SwarmJob


class MyCustomSwarmJob(SwarmJob):
    """
    Example custom job using the new SwarmJob API.

    - Reads prompts from the `input_column_name` column (default: "messages")
    - Produces three output columns: completion, score, current_model
    - No checkpointing/I-O logic here: the runner handles that.
    """

    def __init__(
        self,
        *,
        endpoint: str | None = None,
        model: str = "",
        input_column_name: str = "messages",
        # We'll override outputs to three columns
        output_column_name: str | list[str] = "result",
        checkpoint_interval: int = 16,
        max_concurrency: int = 2,
        retries: int = 5,
        timeout: float = 600,
        **extra_kwargs: Any,
    ):
        # Initialize the base job (creates self.client, stores kwargs, etc.)
        super().__init__(
            endpoint=endpoint,
            model=model,
            input_column_name=input_column_name,
            output_column_name=output_column_name,
            checkpoint_interval=checkpoint_interval,
            max_concurrency=max_concurrency,
            retries=retries,
            timeout=timeout,
            **extra_kwargs,
        )
        # Our job returns 3 values per item
        self.output_column_name = ["completion", "score", "current_model"]

    async def transform_items(self, items: list[Any]) -> list[tuple[str, float, str]]:
        """
        Pure transform: items -> results (same order, same length).
        Each item here is expected to be a prompt string.

        Returns:
            List of tuples: (completion_text, random_score, model_tag)
        """
        # You can pass OpenAI params via job kwargs (e.g., temperature)
        temperature = float(self.kwargs.get("temperature", 0.7))

        results: list[tuple[str, float, str]] = []

        # Note: The executor calls this for single items via `_call_unit`,
        # but we support lists to keep the contract general.
        for prompt in items:
            # Async OpenAI client already configured to hit the swarm endpoint
            resp = await self.client.completions.create(
                model=self.model,
                prompt=prompt,
                **self.kwargs,  # forward any extra OpenAI parameters
            )
            completion_text = resp.choices[0].text or ""
            results.append(
                (
                    completion_text,
                    random.random(),                   # demo score
                    f"{self.model}_{temperature}",     # demo tag
                )
            )

        return results
```

### Run a custom job via CLI

```shell
PYTHONPATH=. domyn-swarm job submit examples.scripts.custom_job:MyCustomJob \
   --config examples/configs/deepseek_r1_distill.yaml \
   --input examples/data/completion.parquet \
   --output results/output.parquet \
   --job-kwargs '{"temperature": 0.2}'
```

### Use a custom job in a script (create a cluster and submit the job manually)

```python
from pathlib import Path
from domyn_swarm import DomynLLMSwarm, DomynLLMSwarmConfig
from mypkg.jobs import MyCustomSwarmJob

cfg = DomynLLMSwarmConfig.read("config.yaml")

with DomynLLMSwarm(cfg=cfg) as swarm:
    job = MyCustomSwarmJob(endpoint=swarm.endpoint, model=swarm.model, max_concurrency=16, temperature=0.2)
    swarm.submit_job(job, input_path=Path("prompts.parquet"), output_path=Path("answers.parquet"))
```

---

## Architecture (in brief)

* **DomynLLMSwarm** (context manager) â†’ brings endpoint up/down, submits work
* **Deployment** â†’ pairs `ServingBackend` + `ComputeBackend`
* **Serving (Slurm/Lepton)** â†’ `create_or_update`, `wait_ready`, `status`, `delete`
* **Compute (Slurm/Lepton)** â†’ `submit`, `wait`, `cancel` (+ defaults helpers)
* **SwarmJob API** â†’ implement `transform_items(items)`; batching/retries/checkpointing provided by the framework
* **State** â†’ saved/loaded locally; rehydrate swarm for later submissions

---

## Contributing

We welcome issues and PRs! Please see:

* `CONTRIBUTING.md` â€” how to propose changes, coding style, DCO/CLA (as applicable)

---

## License

Licensed under the **Apache License, Version 2.0**. See `LICENSE` and `NOTICE`.

---

## Acknowledgements

* Built on **vLLM** and **Ray** (Apacheâ€‘2.0)
* Optional **NVIDIA DGX Cloud Lepton** integration via the Lepton SDK

Happy swarming! ðŸš€
